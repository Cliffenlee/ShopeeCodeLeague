{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Shopee Workshop - Sentiment Analysis [Final].ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J7DjiTx7OjWs",
        "xP3L23QvPGg8",
        "_8esFHgTPd9r",
        "XolbMbwNP3sj",
        "H6i5r0W3QPqr",
        "XERofKqiRqKv",
        "vShQG2qvRz3q",
        "SHqtvkdKR4FI",
        "OPEshFtGOvp5",
        "JA7YtjslR9Z_",
        "mTVvOdj8SICd",
        "IuBLVtOCZ25F",
        "h7rTkvXlawmw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc1bfJ4BTgXA"
      },
      "source": [
        "# **SENTIMENT ANALYSIS WITH TENSORFLOW**\r\n",
        "\r\n",
        "<img src=\"https://inkygoodness.com/wp-content/uploads/2018/01/human_emotions-IG-vanessa.jpg\" width=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7DjiTx7OjWs"
      },
      "source": [
        "# Stage 1: Understand the basic of Tokenizer and Padding of Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP3L23QvPGg8"
      },
      "source": [
        "## Tokenization\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IDsSLuvOl-v"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BntjkLvhOmBX"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0fV0RGIOmDq"
      },
      "source": [
        "# Our sentences\r\n",
        "sentences =[\r\n",
        "            'i love my dog',\r\n",
        "            'I, love my cat'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5eeSUdgPSJc"
      },
      "source": [
        "**num_word** is the maximum number of words we gonna keep. It is ok because we have only two sentences now, but imagine we got hundreds of books to tokenize, with 10 millions unique words and we just want 100 words in all of that if num_word=100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJh8j3MOPCRm"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf_0IXgwVDuZ"
      },
      "source": [
        "fit_on_texts is the method for tokenizer to learn how many unique words in our collection and what kind of frequency of the words appearing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAyLtjz_PCUG"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YhO7aiCVQzr"
      },
      "source": [
        "We can check out how many unique words in the text. \r\n",
        "\r\n",
        "**Question:** Can you guess why some specific words got small index number and some words got high index number?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpc4iG-tPCWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "889eb676-eca5-4d45-a461-d6361607aec0"
      },
      "source": [
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwFsGXrTPWlm"
      },
      "source": [
        "**The tokenizer is smart enough to catch some exceptions like this! Note that dog with \"!\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFa6vy7PPCYy"
      },
      "source": [
        "# Our sentences\r\n",
        "sentences =[\r\n",
        "            'i love my dog',\r\n",
        "            'I, love my <cat>',\r\n",
        "            'You love my dog!'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i3GWrHUPCbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92b7d97c-0283-4d34-e1e1-f8e0b5a65068"
      },
      "source": [
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qfRQf9zUzl6"
      },
      "source": [
        "More info about Tokenization: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7pYtVUSPZWS"
      },
      "source": [
        "You can see how words can be tokenized and tools in Tensorflow can handle that for you.\r\n",
        "\r\n",
        "Now your words are represented by numbers like this then you need to represent your sentences by sequences of numbers in the correct order. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8esFHgTPd9r"
      },
      "source": [
        "## Turning sentences into numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu-k3bmtPfPf"
      },
      "source": [
        "Time to create sequences from sentences!\r\n",
        "\r\n",
        "Let try a different example, this time **these sentences will have different lengths.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GF1MWo6PgsF"
      },
      "source": [
        "# Our sentences\r\n",
        "sentences =[\r\n",
        "            'i love my dog',\r\n",
        "            'I, love my cat',\r\n",
        "            'You love my dog!',\r\n",
        "            'Do you think my dog is amazing?'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPFIYZubPh9L"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100)\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBwOqmv-Pi8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f7543a-a84e-4378-a66b-89e79d516706"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjijVVCGPkoK"
      },
      "source": [
        "**text_to_sequences** will create sequences of tokens representing each sentence. Too easy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSvYn6QsPmEv"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NIk30qFPnDI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2703c6a-4ad9-48cb-8e13-96babdff5554"
      },
      "source": [
        "sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB33LmMKPoDs"
      },
      "source": [
        "You can make sense of the first sentence which is \"I love my dog\" -> [4, 2, 1, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pptqcwL3PqNk"
      },
      "source": [
        "**What about the words that our model never seen before?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9WgbF1ePrZ3"
      },
      "source": [
        "In this example, we will have **new words \"really\" and \"food\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlXWU6otPsdc"
      },
      "source": [
        "# Try with new setences\r\n",
        "test_data=[\r\n",
        "           'i really love my dog',\r\n",
        "           'my dog loves my food'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRTO_WjdPtVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6dd371c-c673-4aaa-d7fa-1a95d78438af"
      },
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_data)\r\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IA8bXjKPuGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "888bb154-3f91-4f2d-8878-a33f7edae925"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdWUvmjAPvpM"
      },
      "source": [
        "So you can imagine that you need a really big word index to handle sentences that are not in the training set.\r\n",
        "\r\n",
        "**In order to not lose the length of sequence like above, there is a trick for that!**\r\n",
        "\r\n",
        "We will create a unique word that would never be in any text like **\"\\<OOV\\>\"**. Then we can replace words which we never seen before with OOV instead!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm52LiO7WU-X"
      },
      "source": [
        "# Let's try again\r\n",
        "test_data=[\r\n",
        "           'i really love my dog',\r\n",
        "           'my dog loves my food'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv2yYjZIPxUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90955578-1d11-4aa8-cc75-0cb3816a2ee1"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDe9NG7RPyF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c96271e-711a-4260-c079-cdbac7950a58"
      },
      "source": [
        "test_seq=tokenizer.texts_to_sequences(test_data)\r\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6T4JsYaP0lJ"
      },
      "source": [
        "Now, all sequences will have the same length of our original sentences. Pretty neat trick right?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgwqxIEmP1u6"
      },
      "source": [
        "Another problem is that how our model can handle sequences with different sizes/lengths \r\n",
        "\r\n",
        "Just like when we train images, inputs of the model are needed to be the same size/length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XolbMbwNP3sj"
      },
      "source": [
        "## Padding sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWcu3Q_rP8aw"
      },
      "source": [
        "# Our sentences\r\n",
        "sentences =[\r\n",
        "            'i love my dog',\r\n",
        "            'I, love my cat',\r\n",
        "            'You love my dog!',\r\n",
        "            'Do you think my dog is amazing?'\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3u3H5aFP9Mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec8d34e-8598-4662-eb80-72b4ccf4f077"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCtmqV0MP-Cv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30340f45-5112-4f47-c308-9fb23a082c2d"
      },
      "source": [
        "sequences=tokenizer.texts_to_sequences(sentences)\r\n",
        "print(sequences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FA2JNJlZANg"
      },
      "source": [
        "More Info about pad_sequences: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUDGjC2-P_LR"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DGuoaZ6QBgi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9de8e6-c921-4549-f869-f4da65933985"
      },
      "source": [
        "padded = pad_sequences(sequences)\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch8ZDtTGQCqU"
      },
      "source": [
        "Nice, so it is padded at the beginining!\r\n",
        "\r\n",
        "What if we want to pad them at the end?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtg_k6AjQD2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ac554fb-0050-48fb-9e66-bdec80028aa2"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post')\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0  0]\n",
            " [ 5  3  2  7  0  0  0]\n",
            " [ 6  3  2  4  0  0  0]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7cESi_3QE74"
      },
      "source": [
        "We can even set the max_len instead of use the maximum length of the longest sentence. \r\n",
        "\r\n",
        "If the sentence is too long for our max_len, we can truncate/remove some words to fit it (truncate=post or pre)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QRwiNuXQGJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbaf17fe-17d9-4da4-e234-de1c91e94c6a"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=6)\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0]\n",
            " [ 5  3  2  7  0  0]\n",
            " [ 6  3  2  4  0  0]\n",
            " [ 8  6  9  2  4 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEaoEaUfQHMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ced45c4-5140-4103-db46-440a534340e6"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=6)\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0]\n",
            " [ 5  3  2  7  0  0]\n",
            " [ 6  3  2  4  0  0]\n",
            " [ 6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_TSYZkEQIYD"
      },
      "source": [
        "Now you know how to tokenize text into numeric values and how to regulaize and pad those text. So the preprocession is done!\r\n",
        "\r\n",
        "Time to train our juicy network model with these representations of sentences to detect if a sentence is positive or negative! However, how can we make sure these numbers be meaningful when it comes to sentiment analysis ? So we need Embedding !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6i5r0W3QPqr"
      },
      "source": [
        "## Embedding Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkahrFDWQ8Sb"
      },
      "source": [
        "The above is your full embedded matrix. We need to find a way to retrieve correct embedded vector for each word and then for each sentence!\r\n",
        "\r\n",
        "![alt text](https://i.imgur.com/z3qObl7.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atgMcwcYQZOY"
      },
      "source": [
        "# Our sentences\r\n",
        "sentences =[\r\n",
        "            'i love my dog',\r\n",
        "            'I, love my cat',\r\n",
        "            'You love my dog!',\r\n",
        "            'Do you think my dog is amazing?'\r\n",
        "]\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(sentences)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Ztnh3caH7w"
      },
      "source": [
        "Nice, we got exactly 12 words in vocab!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT5rBYvqZzW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ca15ee-94bf-4c82-8cda-9317c2800bac"
      },
      "source": [
        "word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'amazing': 11,\n",
              " 'cat': 7,\n",
              " 'do': 8,\n",
              " 'dog': 4,\n",
              " 'i': 5,\n",
              " 'is': 10,\n",
              " 'love': 3,\n",
              " 'my': 2,\n",
              " 'think': 9,\n",
              " 'you': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSTRU5WxQbWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7480fba-e272-4327-e1ae-bde1b7f979b2"
      },
      "source": [
        "sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDEy8FZpQcjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35620139-8554-4431-f250-e7d0c89e0974"
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=6)\r\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0]\n",
            " [ 5  3  2  7  0  0]\n",
            " [ 6  3  2  4  0  0]\n",
            " [ 6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NC7QAeRQdzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4020094-6f14-46aa-a201-696ca9928d57"
      },
      "source": [
        "padded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edP4_ap8Qe8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb8888a-b4b4-4d11-e732-16a4e7fd37e5"
      },
      "source": [
        "first_sentence = padded[0]\r\n",
        "first_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 3, 2, 4, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSCasomcQf_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6915df04-9467-4278-f3db-f93c5e7c6af4"
      },
      "source": [
        "word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'amazing': 11,\n",
              " 'cat': 7,\n",
              " 'do': 8,\n",
              " 'dog': 4,\n",
              " 'i': 5,\n",
              " 'is': 10,\n",
              " 'love': 3,\n",
              " 'my': 2,\n",
              " 'think': 9,\n",
              " 'you': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toh1EVHzQTN0"
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9exWCQ7QUD0"
      },
      "source": [
        "vocab_size = 12 \r\n",
        "embedding_dim = 3 # can be represented for good, bad, crazy\r\n",
        "embedding_layer = layers.Embedding(vocab_size, embedding_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PuR5iJFZWxf"
      },
      "source": [
        "The shape here make senses:\r\n",
        "- We got 12 unique words in our vocab.\r\n",
        "- Each word are represented by 3 number (3 dimension vector)\r\n",
        "\r\n",
        "So the size of embedding layer is 12 by 3 ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xJiVhGbQVLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d62eb87f-9822-4b1d-fb80-7ada8309187f"
      },
      "source": [
        "embedding_layer.input_dim, embedding_layer.output_dim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icwDdQLAZnyD"
      },
      "source": [
        "Let's try to get all embedding vectors from the layer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIH6yi0zQWpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5b5607-dd1f-4f4a-c8c7-91c0d46219de"
      },
      "source": [
        "result = embedding_layer(tf.constant([0,1,2,3,4,5,6,7,8,9,10,11]))\r\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.1744519e-05,  3.8744774e-02,  4.5454454e-02],\n",
              "       [ 3.7292365e-02, -4.5226324e-02, -2.5365019e-02],\n",
              "       [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "       [-3.7309039e-02,  3.7818067e-03,  1.9867215e-02],\n",
              "       [-8.1157684e-03,  1.8775824e-02,  1.3189439e-02],\n",
              "       [-3.1944454e-02, -2.0747567e-02, -4.5926087e-03],\n",
              "       [ 3.6478329e-02,  1.1744276e-03, -7.2725639e-03],\n",
              "       [ 3.3895124e-02,  1.9534323e-02,  2.2506867e-02],\n",
              "       [-4.7945656e-02,  5.8999285e-03,  3.5151947e-02],\n",
              "       [ 3.3130024e-02, -1.9323707e-02,  3.8050737e-02],\n",
              "       [-3.0018687e-03,  1.9134048e-02,  1.2921009e-02],\n",
              "       [-2.5133276e-02,  2.6607402e-03,  1.8285479e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWoDluutAnru",
        "outputId": "f3ea036c-f9b6-460a-e9ab-f8dc42b566d1"
      },
      "source": [
        "word_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'amazing': 11,\n",
              " 'cat': 7,\n",
              " 'do': 8,\n",
              " 'dog': 4,\n",
              " 'i': 5,\n",
              " 'is': 10,\n",
              " 'love': 3,\n",
              " 'my': 2,\n",
              " 'think': 9,\n",
              " 'you': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "getOgsCoQXz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84a153c5-c283-4cda-a978-78ac56d0aed9"
      },
      "source": [
        "result = embedding_layer(tf.constant([0,1,2,1,2,3]))\r\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.1744519e-05,  3.8744774e-02,  4.5454454e-02],\n",
              "       [ 3.7292365e-02, -4.5226324e-02, -2.5365019e-02],\n",
              "       [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "       [ 3.7292365e-02, -4.5226324e-02, -2.5365019e-02],\n",
              "       [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "       [-3.7309039e-02,  3.7818067e-03,  1.9867215e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc4OL-Ata0jR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1852f6d4-676e-49e4-baca-f19e9c21d8a6"
      },
      "source": [
        "first_sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 3, 2, 4, 0, 0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umE6XWKiQhNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b07845a-2f0b-48e0-bcd5-eb105b074dc7"
      },
      "source": [
        "len(first_sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJTutPDzaRGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "766103b3-47d7-4619-c7cb-496485731364"
      },
      "source": [
        "# embedding for 1 sentence\r\n",
        "result=embedding_layer(first_sentence)\r\n",
        "print(result.shape)\r\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-3.1944454e-02, -2.0747567e-02, -4.5926087e-03],\n",
              "       [-3.7309039e-02,  3.7818067e-03,  1.9867215e-02],\n",
              "       [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "       [-8.1157684e-03,  1.8775824e-02,  1.3189439e-02],\n",
              "       [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02],\n",
              "       [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60b5vVlmaXPD"
      },
      "source": [
        "**Question**: Can you make sense why the shape of our output is 4, 6, 3 ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTScQr-nQkSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1ddf51-1229-49c0-ec8c-8602bc38fd2e"
      },
      "source": [
        "# embedding for every sentence\r\n",
        "result=embedding_layer(padded)\r\n",
        "print(result.shape)\r\n",
        "result.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 6, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-3.1944454e-02, -2.0747567e-02, -4.5926087e-03],\n",
              "        [-3.7309039e-02,  3.7818067e-03,  1.9867215e-02],\n",
              "        [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "        [-8.1157684e-03,  1.8775824e-02,  1.3189439e-02],\n",
              "        [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02],\n",
              "        [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02]],\n",
              "\n",
              "       [[-3.1944454e-02, -2.0747567e-02, -4.5926087e-03],\n",
              "        [-3.7309039e-02,  3.7818067e-03,  1.9867215e-02],\n",
              "        [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "        [ 3.3895124e-02,  1.9534323e-02,  2.2506867e-02],\n",
              "        [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02],\n",
              "        [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02]],\n",
              "\n",
              "       [[ 3.6478329e-02,  1.1744276e-03, -7.2725639e-03],\n",
              "        [-3.7309039e-02,  3.7818067e-03,  1.9867215e-02],\n",
              "        [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "        [-8.1157684e-03,  1.8775824e-02,  1.3189439e-02],\n",
              "        [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02],\n",
              "        [-2.1744519e-05,  3.8744774e-02,  4.5454454e-02]],\n",
              "\n",
              "       [[ 3.6478329e-02,  1.1744276e-03, -7.2725639e-03],\n",
              "        [ 3.3130024e-02, -1.9323707e-02,  3.8050737e-02],\n",
              "        [-5.4663531e-03,  4.1066814e-02, -3.5211097e-02],\n",
              "        [-8.1157684e-03,  1.8775824e-02,  1.3189439e-02],\n",
              "        [-3.0018687e-03,  1.9134048e-02,  1.2921009e-02],\n",
              "        [-2.5133276e-02,  2.6607402e-03,  1.8285479e-02]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vMzXLw-a4JC"
      },
      "source": [
        "Now, you have successfuly learn how to convert the sentences of text into numeric tokens and for each numeric token, you know how to retrieve the corresponding word embedding vector!\r\n",
        "\r\n",
        "Now time to learn how to build a neural network to fetch a real IMDB dataset for it to train and update these values in embedding layers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XERofKqiRqKv"
      },
      "source": [
        "# Stage 2: IMBD Sentiment Analysis with simple neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vShQG2qvRz3q"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCFIrMd0YJXQ"
      },
      "source": [
        "Same old libraries like above ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1rjXBbYBkkF"
      },
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmMnXvDLYM2O"
      },
      "source": [
        "By now, you can start to make sense of these parameters we set at the beginning. Feel free to tune your model by changing these parameters!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EZwVpzrB4NB"
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZInTeqTZ0FG"
      },
      "source": [
        "Let's import tensorflow datasets library and download IDMB text reviews dataset!\r\n",
        "\r\n",
        "Here are more info of IMDB dataset we got from Tensorflow: https://www.tensorflow.org/datasets/catalog/imdb_reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS0HtPssB5Eo"
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jd1uFlZCIqv"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "\n",
        "# str(s.tonumpy()) is needed in Python3 instead of just s.numpy()\n",
        "for s,l in train_data:\n",
        "  training_sentences.append(s.numpy().decode('utf8'))\n",
        "  training_labels.append(l.numpy())\n",
        "  \n",
        "for s,l in test_data:\n",
        "  testing_sentences.append(s.numpy().decode('utf8'))\n",
        "  testing_labels.append(l.numpy())\n",
        "  \n",
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final = np.array(testing_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfcMT0iULylA"
      },
      "source": [
        "len(training_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euub4rPAayrj"
      },
      "source": [
        "print(\"Sentence:\", training_sentences[0])\n",
        "print(\"Label:\", training_labels[0])\n",
        "print(\"---------------------------------\")\n",
        "print(\"Sentence:\", training_sentences[1])\n",
        "print(\"Label:\", training_labels[1])\n",
        "print(\"---------------------------------\")\n",
        "print(\"Sentence:\", training_sentences[3])\n",
        "print(\"Label:\", training_labels[3])\n",
        "print(\"---------------------------------\")\n",
        "print(\"Sentence:\", training_sentences[5])\n",
        "print(\"Label:\", training_labels[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoIZ9_rsbGlT"
      },
      "source": [
        "Time to tokenize our sentences and pad them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzePDzHBCJ4b"
      },
      "source": [
        "# For Training Data:\n",
        "\n",
        "# create a tokenizer with num_words and oov_token attributes\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "\n",
        "# use that tokenizer to fit the training sentences we got above\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "# retrieve the word_index back from the tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# use the tokenizer we have fitted on the training sentences and create encoded sequences of index of training setences\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "\n",
        "# remember to pad the sequences for them to be on the same length with max_len and truncating attributes\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "\n",
        "# For Testing Data: \n",
        "\n",
        "# we only use the same tokenize we got above and create encoded sequences of index of test setences\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "\n",
        "# remember to pad the sequences for them to be on the same length with max_len and truncating attributes\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHtp3VMin6FQ"
      },
      "source": [
        "If you implements the above code correctly, this decode should give you back the original text from encoded sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPcmXDrTCKw7"
      },
      "source": [
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(\"Original:\", training_sentences[0])\n",
        "print(\"Tokenize:\", sequences[0])\n",
        "print(\"Reduce or Padded:\", padded[0])\n",
        "print(\"Decode:\", decode_review(padded[0]))\n",
        "print(\"Label:\", training_labels_final[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afnHHEkBcfFT"
      },
      "source": [
        "It's always good to visualize training and validation loss or accuracy after training the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgHzLvKaWnI-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHqtvkdKR4FI"
      },
      "source": [
        "## Embedding layer with Flatten Layer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of1xlRYcbu-J"
      },
      "source": [
        "The process of training the model have repeated 4 steps:\r\n",
        "\r\n",
        "1. Forward Propagation (Plug in the word tokens of sentences and output one number for sentiment result - output)\r\n",
        "\r\n",
        "2. Compare the output to the expected output (label) by calculating the loss function (measuring how bad is your output of your model)\r\n",
        "\r\n",
        "3. Back Propagation (Calculate how much you need to update your parameter values in your model so the next step you do the forward propagation will be slightly better)\r\n",
        "\r\n",
        "4. Gradient Descent (Actually take the values from step 3 and update those parameters in the model)\r\n",
        "\r\n",
        "After training the model for many epoches, the model will have good parameters to achieve high accuracy and low loss! :)\r\n",
        "\r\n",
        "So define the model as below to tell the model how to do in **step 1** (forward propagation).\r\n",
        "\r\n",
        "Next loss function \"binary_crossentropy\" is loss function to measure how bad the output compared to the expected output in **step 2**.\r\n",
        "\r\n",
        "Optimizer is the function is to define how you should update the parameters in **step 4**.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y13BoZ4mCLj8"
      },
      "source": [
        "# YOUR CODE\n",
        "# Build model with Flatten(), a Fully Connected Layer like 8 neurons and the last layer is a Fully Connected Layer with 1 neuron with activation is 'sigmoid'\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # your code\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqOzHbu5dUlx"
      },
      "source": [
        "To train the model, you can simply call the method fit ;) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EJAosxWCMjT"
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlaIIwiFdZPU"
      },
      "source": [
        "Plot how well your model doing after training!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPUac50sWpnr"
      },
      "source": [
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDsEUGlbdhyz"
      },
      "source": [
        "I won't discuss about overfitting and underfitting in this workshop because that topic alone can take another workshop to do :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AP_Z6-dRn3o"
      },
      "source": [
        "### Prediction sentiment of some sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeffQj92Rmg1"
      },
      "source": [
        "# YOU CAN TRY YOUR OWN EXAMPLES\n",
        "# Hopefully, the first and third sentences are positive and middle one is negative\n",
        "\n",
        "sentence = [\"I really think this is amazing. honest.\", \"It sucks and so bad\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "print(\"Sequences:\", sequences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(\"Padded:\", padded)\n",
        "print(\"Prediction:\", model.predict(padded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCO-BVVUUQnT"
      },
      "source": [
        "### [Fun] Visualize your embedding vectors in 3 dimension space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mMm61sGUoJC"
      },
      "source": [
        "You need to export the Embedding layer into vecs and meta files to visualise \n",
        "\n",
        "Run this code to export the values of vectors in embedding (vecs.tsv) and coressponding words (meta.tsv). Thus, you will have two files in total.\n",
        "\n",
        "Remember to click \"Allow to download multiple files\" on Chrome to download two files at the same time!\n",
        "\n",
        "Open http://projector.tensorflow.org/ and load those two files you just download so see the visualization in 3D or 2D of your word embedding!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eG_tL2cfVGuf"
      },
      "source": [
        "import io\n",
        "def export_embedding_tsv(model):\n",
        "  e = model.layers[0]\n",
        "  weights = e.get_weights()[0]\n",
        "  #print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
        "\n",
        "  out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "  out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "  for word_num in range(1, vocab_size):\n",
        "    word = reverse_word_index[word_num]\n",
        "    embeddings = weights[word_num]\n",
        "    out_m.write(word + \"\\n\")\n",
        "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
        "  out_v.close()\n",
        "  out_m.close()\n",
        "\n",
        "  try:\n",
        "    from google.colab import files\n",
        "  except ImportError:\n",
        "    pass\n",
        "  else:\n",
        "    files.download('vecs.tsv')\n",
        "    files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THNtCHZDVc7D"
      },
      "source": [
        "export_embedding_tsv(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPEshFtGOvp5"
      },
      "source": [
        "## Extra Models to play around with (Optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA7YtjslR9Z_"
      },
      "source": [
        "### Embedding layer with Global Average Pooling Layer\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6l0_LoyRetR"
      },
      "source": [
        "![](https://i.imgur.com/H0Gh7wA.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su5KdtXEWESa"
      },
      "source": [
        "# YOUR CODE\n",
        "# Use Global Average Pooling 1D and 2 Dense Layers with the last layer is one neuron and activation is sigmoid. \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # your code\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    \n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbEcz-Y2WWTv"
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhnD_eqTWq22"
      },
      "source": [
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f75qGSBhS3N"
      },
      "source": [
        "Do you notice anything different from the last model about speed and accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTVvOdj8SICd"
      },
      "source": [
        "### Embedding layer with a LSTM layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY90QbBgSMKC"
      },
      "source": [
        "# YOUR CODE\n",
        "# Use LSTM layer with 2 Dense Layers followed it\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # your code\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    \n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWlImbp_SPDW"
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsPYMPgnSQZV"
      },
      "source": [
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuBLVtOCZ25F"
      },
      "source": [
        "### Embedding layer with a Bidirectional LSTM layer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tk6yN6bZ8ht"
      },
      "source": [
        "# YOUR CODE\n",
        "# Use bidirectional LSTM with 2 dense layers\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # your code\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    \n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZSjpBVwaZOU"
      },
      "source": [
        "num_epochs = 10\n",
        "history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MamEz8hKaeE6"
      },
      "source": [
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7rTkvXlawmw"
      },
      "source": [
        "### Embedding layer with multiple Bidirectional LSTM layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XANJ8jNNbJpN"
      },
      "source": [
        "# YOUR CODE\n",
        "# Use stacked bidirectional LSTM (2 bidirectional LSTM stacked on each other) with two dense layers \n",
        "# be mindful about return_sequence \n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # your code\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    \n",
        "])\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "num_epochs = 10\n",
        "history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}